{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a00269d",
   "metadata": {},
   "source": [
    "# Sentiment Analysis with Word2Vec and SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d8b996d",
   "metadata": {},
   "source": [
    "### Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cef063ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "train_samples = pickle.load(open('./data/train_samples.pkl', 'rb'))\n",
    "train_labels = pickle.load(open('./data/train_labels.pkl', 'rb'))\n",
    "\n",
    "split_percentage = .85\n",
    "split_threshold = int(split_percentage * len(train_labels))\n",
    "val_samples = train_samples[split_threshold:]\n",
    "train_samples = train_samples[:split_threshold]\n",
    "val_labels = train_labels[split_threshold:]\n",
    "train_labels = train_labels[:split_threshold]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "131d1cfa",
   "metadata": {},
   "source": [
    "### Loading the Word2Vec model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fcaa4c0",
   "metadata": {},
   "source": [
    "Download the word2vec model from https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit?usp=sharing and unzip it in the current directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de3f6270",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format('./GoogleNews-vectors-negative300.bin', binary = True)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "128807f1",
   "metadata": {},
   "source": [
    "### Preprocessing our input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2aa26bf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/tonio/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "/tmp/ipykernel_1717882/3135882871.py:12: DeprecationWarning: Call to deprecated `word_vec` (Use get_vector instead).\n",
      "  representation += model.word_vec(word)\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "import numpy as np\n",
    "\n",
    "def preprocess(review):\n",
    "    representation = np.zeros(300)\n",
    "    n_words = 0\n",
    "    for sentence in sent_tokenize(review):\n",
    "        for word in word_tokenize(sentence):\n",
    "            try:\n",
    "                representation += model.word_vec(word)\n",
    "                n_words += 1\n",
    "            except:\n",
    "                pass\n",
    "    \n",
    "    return representation / n_words\n",
    "\n",
    "train_data = np.array([preprocess(review) for review in train_samples]).astype(np.float32)\n",
    "val_data = np.array([preprocess(review) for review in val_samples]).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6fd1d203",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1445, 300)\n"
     ]
    }
   ],
   "source": [
    "print(train_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b357a457",
   "metadata": {},
   "source": [
    "## Pytorch implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d53e53",
   "metadata": {},
   "source": [
    "### Basic setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a1b04cba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.2717, -1.6983],\n",
      "        [-1.5204, -1.6406],\n",
      "        [-1.6709, -1.9722],\n",
      "        [-1.8916, -1.7740],\n",
      "        [-1.5462, -2.1345],\n",
      "        [-1.9890, -2.0022],\n",
      "        [-1.8241, -2.2094],\n",
      "        [-1.9009, -1.7700],\n",
      "        [-1.5751, -1.8321],\n",
      "        [-1.6853, -1.9322]], grad_fn=<AddBackward0>)\n",
      "tensor(0.7706, grad_fn=<NllLossBackward>)\n",
      "None\n",
      "tensor([ 0.1515, -0.1515])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "w = torch.nn.Parameter(torch.randn(300, 2))\n",
    "b = torch.nn.Parameter(torch.randn(2,))\n",
    "\n",
    "input = torch.Tensor(train_data[:10])\n",
    "target = torch.Tensor(train_labels[:10]).long()\n",
    "\n",
    "output = torch.matmul(input, w) + b\n",
    "\n",
    "print(output)\n",
    "loss_function = torch.nn.CrossEntropyLoss()\n",
    "loss = loss_function(output, target)\n",
    "print(loss)\n",
    "\n",
    "print(b.grad)\n",
    "loss.backward()\n",
    "print(b.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2544ea8f",
   "metadata": {},
   "source": [
    "### Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b0ff66c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "w = torch.nn.Parameter(torch.randn(300, 2))\n",
    "b = torch.nn.Parameter(torch.randn(2,))\n",
    "loss_function = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(lr = 1e-2, params = [w, b])\n",
    "\n",
    "batch_size = 64\n",
    "for epoch_n in range(100):\n",
    "    for sample_n in range(0, len(train_labels), batch_size):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        input = torch.Tensor(train_data[sample_n: sample_n + batch_size])\n",
    "        target = torch.Tensor(train_labels[sample_n: sample_n + batch_size]).long()\n",
    "\n",
    "        output = torch.matmul(input, w) + b\n",
    "        loss = loss_function(output, target)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2e53ecc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8235294117647058\n"
     ]
    }
   ],
   "source": [
    "val_predictions = torch.matmul(torch.Tensor(val_data), w) + b\n",
    "val_predictions = np.argmax(val_predictions.detach().numpy(), axis = 1)\n",
    "print(np.mean(val_predictions == val_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72ae197d",
   "metadata": {},
   "source": [
    "### Introducing modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e7271f62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8313725490196079\n"
     ]
    }
   ],
   "source": [
    "class Model(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.w = torch.nn.Parameter(torch.randn(300, 2))\n",
    "        self.b = torch.nn.Parameter(torch.randn(2,))\n",
    "    \n",
    "    def forward(self, input):\n",
    "        return torch.matmul(input, self.w) + self.b\n",
    "    \n",
    "model = Model()\n",
    "loss_function = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(lr = 1e-2, params = model.parameters())\n",
    "\n",
    "batch_size = 64\n",
    "for epoch_n in range(100):   \n",
    "    for sample_n in range(0, len(train_labels), batch_size):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        input = torch.Tensor(train_data[sample_n: sample_n + batch_size])\n",
    "        target = torch.Tensor(train_labels[sample_n: sample_n + batch_size]).long()\n",
    "\n",
    "        output = model(input)\n",
    "        loss = loss_function(output, target)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "val_predictions = model(torch.Tensor(val_data))\n",
    "val_predictions = np.argmax(val_predictions.detach().numpy(), axis = 1)\n",
    "print(np.mean(val_predictions == val_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3de410fc",
   "metadata": {},
   "source": [
    "### Introducing layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "11cdbf8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8392156862745098\n"
     ]
    }
   ],
   "source": [
    "class Model(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.layer = torch.nn.Linear(300, 2)\n",
    "    \n",
    "    def forward(self, input):\n",
    "        return self.layer(input)\n",
    "    \n",
    "model = Model()\n",
    "loss_function = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(lr = 1e-2, params = model.parameters())\n",
    "\n",
    "batch_size = 64\n",
    "for epoch_n in range(100):\n",
    "    for sample_n in range(0, len(train_labels), batch_size):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        input = torch.Tensor(train_data[sample_n: sample_n + batch_size])\n",
    "        target = torch.Tensor(train_labels[sample_n: sample_n + batch_size]).long()\n",
    "\n",
    "        output = model(input)\n",
    "        loss = loss_function(output, target)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "val_predictions = model(torch.Tensor(val_data))\n",
    "val_predictions = np.argmax(val_predictions.detach().numpy(), axis = 1)\n",
    "print(np.mean(val_predictions == val_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18e8f8e4",
   "metadata": {},
   "source": [
    "### Sequential model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b81508cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8627450980392157\n"
     ]
    }
   ],
   "source": [
    "class Model(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.layer = torch.nn.Sequential(\n",
    "            torch.nn.Linear(300, 256),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(256, 2)\n",
    "        )\n",
    "    \n",
    "    def forward(self, input):\n",
    "        return self.layer(input)\n",
    "    \n",
    "model = Model()\n",
    "loss_function = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(lr = 1e-3, params = model.parameters())\n",
    "\n",
    "batch_size = 64\n",
    "for epoch_n in range(100):\n",
    "    for sample_n in range(0, len(train_labels), batch_size):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        input = torch.Tensor(train_data[sample_n: sample_n + batch_size])\n",
    "        target = torch.Tensor(train_labels[sample_n: sample_n + batch_size]).long()\n",
    "\n",
    "        output = model(input)\n",
    "        loss = loss_function(output, target)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "val_predictions = model(torch.Tensor(val_data))\n",
    "val_predictions = np.argmax(val_predictions.detach().numpy(), axis = 1)\n",
    "print(np.mean(val_predictions == val_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48ae9abe",
   "metadata": {},
   "source": [
    "### Introducing datasets and dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4772017b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8470588235294118\n"
     ]
    }
   ],
   "source": [
    "class Model(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.layer = torch.nn.Sequential(\n",
    "            torch.nn.Linear(300, 256),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(256, 2)\n",
    "        )\n",
    "    \n",
    "    def forward(self, input):\n",
    "        return self.layer(input)\n",
    "    \n",
    "    \n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, split):\n",
    "        super().__init__()\n",
    "        \n",
    "        if split == 'train':\n",
    "            self.data = train_data\n",
    "            self.labels = train_labels\n",
    "        else:\n",
    "            self.data = val_data\n",
    "            self.labels = val_labels\n",
    "        \n",
    "    def __getitem__(self, sample_n):\n",
    "        return self.data[sample_n], self.labels[sample_n]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    \n",
    "model = Model()\n",
    "loss_function = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(lr = 1e-3, params = model.parameters())\n",
    "\n",
    "batch_size = 64\n",
    "train_dataset = Dataset(split = 'train')\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    shuffle = True,\n",
    "    batch_size = batch_size,\n",
    "    num_workers = 10,\n",
    ")\n",
    "val_dataset = Dataset(split = 'val')\n",
    "val_dataloader = torch.utils.data.DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size = batch_size,\n",
    "    num_workers = 10,\n",
    ")\n",
    "\n",
    "for epoch_n in range(100):\n",
    "    for batch in train_dataloader:\n",
    "        input, target = batch\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        output = model(input)\n",
    "        loss = loss_function(output, target)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "val_predictions = model(torch.Tensor(val_data))\n",
    "val_predictions = np.argmax(val_predictions.detach().numpy(), axis = 1)\n",
    "print(np.mean(val_predictions == val_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5188f792",
   "metadata": {},
   "source": [
    "### Training on GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "42d54036",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8431372549019608\n"
     ]
    }
   ],
   "source": [
    "class Model(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.layer = torch.nn.Sequential(\n",
    "            torch.nn.Linear(300, 256),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(256, 2)\n",
    "        )\n",
    "    \n",
    "    def forward(self, input):\n",
    "        return self.layer(input)\n",
    "    \n",
    "    \n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, split):\n",
    "        super().__init__()\n",
    "        \n",
    "        if split == 'train':\n",
    "            self.data = train_data\n",
    "            self.labels = train_labels\n",
    "        else:\n",
    "            self.data = val_data\n",
    "            self.labels = val_labels\n",
    "        \n",
    "    def __getitem__(self, sample_n):\n",
    "        return self.data[sample_n], self.labels[sample_n]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "device = torch.device('cuda')\n",
    "    \n",
    "model = Model().to(device)\n",
    "loss_function = torch.nn.CrossEntropyLoss().to(device)\n",
    "optimizer = torch.optim.Adam(lr = 1e-3, params = model.parameters())\n",
    "\n",
    "batch_size = 256\n",
    "train_dataset = Dataset(split = 'train')\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    shuffle = True,\n",
    "    batch_size = batch_size,\n",
    "    num_workers = 10,\n",
    ")\n",
    "val_dataset = Dataset(split = 'val')\n",
    "val_dataloader = torch.utils.data.DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size = batch_size,\n",
    "    num_workers = 10,\n",
    ")\n",
    "\n",
    "for epoch_n in range(100):\n",
    "    for batch in train_dataloader:\n",
    "        input, target = batch\n",
    "        input = input.to(device)\n",
    "        target = target.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        output = model(input)\n",
    "        loss = loss_function(output, target)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "val_predictions = model(torch.Tensor(val_data).to(device))\n",
    "val_predictions = np.argmax(val_predictions.detach().cpu().numpy(), axis = 1)\n",
    "print(np.mean(val_predictions == val_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31311657",
   "metadata": {},
   "source": [
    "### Evaluation procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f56ee3c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.49411764705882355\n",
      "0.6941176470588235\n",
      "0.7294117647058823\n",
      "0.7098039215686275\n",
      "0.6705882352941176\n",
      "0.7372549019607844\n",
      "0.7294117647058823\n",
      "0.7490196078431373\n",
      "0.7294117647058823\n",
      "0.7686274509803922\n",
      "0.7764705882352941\n",
      "0.7254901960784313\n",
      "0.7607843137254902\n",
      "0.788235294117647\n",
      "0.788235294117647\n",
      "0.7803921568627451\n",
      "0.7764705882352941\n",
      "0.7803921568627451\n",
      "0.7843137254901961\n",
      "0.7803921568627451\n",
      "0.7843137254901961\n",
      "0.7803921568627451\n",
      "0.7607843137254902\n",
      "0.7725490196078432\n",
      "0.7764705882352941\n",
      "0.7764705882352941\n",
      "0.792156862745098\n",
      "0.792156862745098\n",
      "0.792156862745098\n",
      "0.7843137254901961\n",
      "0.807843137254902\n",
      "0.788235294117647\n",
      "0.8\n",
      "0.792156862745098\n",
      "0.8\n",
      "0.8\n",
      "0.803921568627451\n",
      "0.803921568627451\n",
      "0.807843137254902\n",
      "0.807843137254902\n",
      "0.807843137254902\n",
      "0.8235294117647058\n",
      "0.8235294117647058\n",
      "0.8196078431372549\n",
      "0.8313725490196079\n",
      "0.8274509803921568\n",
      "0.8352941176470589\n",
      "0.8352941176470589\n",
      "0.8235294117647058\n",
      "0.8196078431372549\n",
      "0.8274509803921568\n",
      "0.8274509803921568\n",
      "0.8352941176470589\n",
      "0.8470588235294118\n",
      "0.8235294117647058\n",
      "0.8274509803921568\n",
      "0.8509803921568627\n",
      "0.8549019607843137\n",
      "0.8470588235294118\n",
      "0.8431372549019608\n",
      "0.8431372549019608\n",
      "0.8313725490196079\n",
      "0.8235294117647058\n",
      "0.8509803921568627\n",
      "0.8392156862745098\n",
      "0.8156862745098039\n",
      "0.8274509803921568\n",
      "0.8549019607843137\n",
      "0.8470588235294118\n",
      "0.8549019607843137\n",
      "0.8588235294117647\n",
      "0.8549019607843137\n",
      "0.8549019607843137\n",
      "0.8588235294117647\n",
      "0.8588235294117647\n",
      "0.8627450980392157\n",
      "0.8588235294117647\n",
      "0.8627450980392157\n",
      "0.8392156862745098\n",
      "0.8666666666666667\n",
      "0.8470588235294118\n",
      "0.8509803921568627\n",
      "0.8666666666666667\n",
      "0.8627450980392157\n",
      "0.8666666666666667\n",
      "0.8705882352941177\n",
      "0.8392156862745098\n",
      "0.8627450980392157\n",
      "0.8509803921568627\n",
      "0.8666666666666667\n",
      "0.8313725490196079\n",
      "0.8352941176470589\n",
      "0.8666666666666667\n",
      "0.8549019607843137\n",
      "0.8392156862745098\n",
      "0.8549019607843137\n",
      "0.8509803921568627\n",
      "0.8509803921568627\n",
      "0.8549019607843137\n",
      "0.8745098039215686\n"
     ]
    }
   ],
   "source": [
    "class Model(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.layer = torch.nn.Sequential(\n",
    "            torch.nn.Linear(300, 256),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(256, 2)\n",
    "        )\n",
    "    \n",
    "    def forward(self, input):\n",
    "        return self.layer(input)\n",
    "    \n",
    "    \n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, split):\n",
    "        super().__init__()\n",
    "        \n",
    "        if split == 'train':\n",
    "            self.data = train_data\n",
    "            self.labels = train_labels\n",
    "        else:\n",
    "            self.data = val_data\n",
    "            self.labels = val_labels\n",
    "        \n",
    "    def __getitem__(self, sample_n):\n",
    "        return self.data[sample_n], self.labels[sample_n]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "device = torch.device('cuda')\n",
    "    \n",
    "model = Model().to(device)\n",
    "loss_function = torch.nn.CrossEntropyLoss().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-3)\n",
    "\n",
    "batch_size = 256\n",
    "train_dataset = Dataset(split = 'train')\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    shuffle = True,\n",
    "    batch_size = batch_size,\n",
    "    num_workers = 10,\n",
    ")\n",
    "val_dataset = Dataset(split = 'val')\n",
    "val_dataloader = torch.utils.data.DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size = batch_size,\n",
    "    num_workers = 10,\n",
    ")\n",
    "\n",
    "for epoch_n in range(100):\n",
    "    model.train()\n",
    "    for batch in train_dataloader:\n",
    "        input, target = batch\n",
    "        input = input.to(device)\n",
    "        target = target.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        output = model(input)\n",
    "        loss = loss_function(output, target)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    model.eval()\n",
    "    val_acc = []\n",
    "    for batch in val_dataloader:\n",
    "        input, target = batch\n",
    "        input = input.to(device)\n",
    "        target = target.to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            output = model(input)\n",
    "        \n",
    "        val_acc += (output.argmax(1) == target).tolist()\n",
    "    print(np.mean(val_acc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
